<!DOCTYPE html>
<html>
<body>
<p>Ethan Goldfarb<br>
ethankgoldfarb@gmail.com</p>

<p>here's a list of the papers I've been reading recently:
<p style="font-size: 10px">Notes:<br> 
what "read" in this instance constitutes ranges from "skimmed" to "fully annotated". Most are fully read and synthesized but not annotated. Ask me if curious about a particular paper.<br>
* indicates I liked a paper or found it notable. Lack of a * does not indicate otherwise.<br>
Papers were read during or before the listed month (some years ago)<br>
This list is not comprehensive, just what has been recorded on a subset of my devices (many imply familiarity with papers not listed)<br>
I am most likely going to break these into topic groups given the... sprawl, particularly in September. This is v0.1!</p>
<br>

Favorites:<br>
<a href="https://arxiv.org/pdf/2203.15556">* Training Compute-Optimal Large Language Models</a><br>
<a href="https://arxiv.org/pdf/2103.00020">* Learning Transferable Visual Models From Natural Language Supervision</a><br>
<a href="https://arxiv.org/abs/2106.01345">* Decision Transformer: Reinforcement Learning via Sequence Modeling</a><br>
<a href="https://arxiv.org/abs/1904.00962">* Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</a><br>
<a href="https://arxiv.org/pdf/2104.06069">* 1-Bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMBâ€™s Convergence Speed</a><br>
<a href="https://arxiv.org/pdf/1910.02054">* ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a><br>
<a href="https://arxiv.org/pdf/2010.11929">* An Image Is Worth 16X16 Words: Transformers for Image Recognition at Scale</a><br>
<a href="https://arxiv.org/pdf/2207.09238">* Formal Algorithms for Transformers</a><br>
<br>
<br>

2023:<br>
<br>
  
March (in progress):<br>
<a href="https://arxiv.org/pdf/2204.12260">Masked Spectrogram Modeling using Masked Autoencoders for Learning General-purpose Audio Representation</a><br>
<a href="https://arxiv.org/abs/2110.05069">Efficient Training of Audio Transformers with Patchout</a><br>
<br>

February:<br>
<a href="https://arxiv.org/abs/2301.11325">MusicLM: Generating Music From Text</a><br>
<a href="https://arxiv.org/pdf/2006.11477.pdf">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</a><br>
<a href="https://arxiv.org/pdf/2206.04850.pdf">Feature-informed Embedding Space Regularization For Audio Classification</a><br>
<a href="https://arxiv.org/pdf/1904.05862.pdf">Wav2vec: Unsupervised Pre-Training for Speech Recognition</a><br>
<a href="https://ai.facebook.com/research/publications/efficient-self-supervised-learning-with-contextualized-target-representations-for-vision-speech-and-language">Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language</a><br>
<a href="https://lilianweng.github.io/posts/2023-01-10-inference-optimization/">Large Transformer Model Inference Optimization</a><br>
<a href="https://arxiv.org/pdf/1905.12334">Mixed Precision Training With 8-bit Floating Point</a><br>
<a href="https://arxiv.org/pdf/2111.11429">Benchmarking Detection Transfer Learning with Vision Transformers</a><br>
<a href="https://arxiv.org/pdf/2106.04550">DETReg: Unsupervised Pretraining with Region Priors for Object Detection</a><br>
<a href="https://arxiv.org/abs/2105.04906">VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning</a><br>
<a href="https://arxiv.org/pdf/2010.04159">Deformable DETR: Deformable Transformers for End-to-End Object Detection</a><br>
<a href="https://arxiv.org/pdf/1703.06211">Deformable Convolutional Networks</a><br>
<a href="https://arxiv.org/pdf/2005.12872">End-to-End Object Detection with Transformers</a><br>
<a href="https://arxiv.org/pdf/2301.03044">A Survey on Transformers in Reinforcement Learning</a><br>
<a href="https://www.ias.informatik.tu-darmstadt.de/uploads/Teaching/HumanoidRoboticsSeminar/HR_Report_21_22_Timo_Imhof_Decision_Transformer.pdf">A Review of the Decision Transformer Architecture</a><br>
<br>

January:<br>
<a href="https://arxiv.org/pdf/2106.08254">BEIT: BERT Pre-Training of Image Transformers</a><br>
<a href="https://arxiv.org/abs/2009.14783">HetSeq: Distributed GPU Training on Heterogeneous Infrastructure</a><br>
<a href="https://arxiv.org/pdf/1701.05517">PixelCNN++: Improving the PixelCNN With Discretized Logistic Mixture Likelihood and Other Modifications</a><br>
<a href="https://arxiv.org/pdf/2102.12092">Zero-Shot Text-to-Image Generation</a><br>
<a href="https://arxiv.org/abs/2105.05633">Segmenter: Transformer for Semantic Segmentation</a><br>
<a href="https://arxiv.org/abs/2111.06377">* Masked Autoencoders Are Scalable Vision Learners</a><br>
<a href="https://arxiv.org/pdf/2210.10615">* A Unified View of Masked Image Modeling</a><br>
<a href="https://arxiv.org/abs/2202.03555">data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language</a><br>
<a href="https://arxiv.org/pdf/2210.10615">Online Decision Transformer</a><br>
<a href="http://arxiv.org/abs/2301.10492v1">Flow-guided Semi-supervised Video Object Segmentation</a><br>
<a href="http://arxiv.org/abs/2301.11310v1">Learning Good Features to Transfer Across Tasks and Domains</a><br>
<a href="http://arxiv.org/abs/2301.10743v1">Tighter Bounds on the Expressivity of Transformer Encoders
</a><br>
<a href="http://arxiv.org/abs/2301.10750v1">Out of Distribution Performance of State of Art Vision Model</a><br>
<a href="http://arxiv.org/abs/2301.09869v1">Image Super-Resolution using Efficient Striped Window Transformer</a><br>
<a href="http://arxiv.org/abs/2301.11320v1">Cut and Learn for Unsupervised Object Detection and Instance Segmentation</a><br>
<a href="https://paperswithcode.com/paper/image-as-a-foreign-language-beit-pretraining">Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks
</a><br>
<a href="https://dl.acm.org/doi/10.1145/3386367.3432728">Optimizing Distributed Training Deployment in Heterogeneous GPU Clusters</a><br>
<a href="https://dl.acm.org/doi/10.1007/s11227-019-02845-2">BOA: Batch Orchestration Algorithm for Straggler Mitigation of Distributed Dl Training in Heterogeneous GPU Cluster</a><br>
<a href="https://arxiv.org/pdf/2205.11487.pdf">Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</a><br>
<a href="https://arxiv.org/pdf/2211.01324.pdf">eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers</a><br>
<a href="https://arxiv.org/pdf/2112.10752.pdf">High-Resolution Image Synthesis with Latent Diffusion Models</a><br>
<a href="https://arxiv.org/pdf/2105.05233.pdf">Diffusion Models Beat GANs on Image Synthesis</a><br>
<a href="https://arxiv.org/pdf/1804.08838.pdf">Measuring the Intrinsic Dimension of Objective Landscapes</a><br>
<a href="https://arxiv.org/pdf/1911.12360.pdf">How Much Over-Parameterization is Sufficient to Learn Deep ReLu Networks?</a><br>
<a href="https://arxiv.org/pdf/2210.14199.pdf">Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models</a><br>
<a href="https://arxiv.org/pdf/2211.14699.pdf">A Theoretical Study of Inductive Biases in Contrastive Learning</a><br>
<a href="https://arxiv.org/pdf/2206.10139.pdf">Insights into Pre-training via Simpler Synthetic Tasks</a><br>
<a href="https://arxiv.org/abs/1601.01280">Language to Logical Form with Neural Attention</a><br>
<a href="https://arxiv.org/pdf/2201.07348.pdf">Learning Tensor Representations for Meta-Learning</a><br>
<a href="https://arxiv.org/pdf/2105.05233.pdf">Diffusion Models Beat GANs on Image Synthesis</a><br>
<a href="https://arxiv.org/pdf/2206.08853.pdf">MINEDOJO: Building Open-Ended Embodied Agents with Internet-Scale Knowledge</a><br>
<a href="https://openreview.net/pdf?id=M3Y74vmsMcY">LAION-5B: An open large-scale dataset for training next generation image-text models</a><br">
<a href="https://arxiv.org/pdf/1503.03585.pdf">* Deep Unsupervised Learning using Nonequilibrium Thermodynamics</a><br>
<a href="https://arxiv.org/pdf/2106.02039.pdf">Offline Reinforcement Learning as One Big Sequence Modeling Problem</a><br>
<a href="https://arxiv.org/pdf/2112.10752.pdf">High-Resolution Image Synthesis with Latent Diffusion Models</a><br>
<a href="https://arxiv.org/pdf/2204.06125.pdf">Hierarchical Text-Conditional Image Generation with CLIP Latents</a><br>
<br>
<br>

2022:<br>
December:<br>
<a href="https://arxiv.org/abs/2108.04552">The Benefits of Implicit Regularization from SGD in Least Squares Problems</a><br>
<a href="https://arxiv.org/abs/2106.01345">* Decision Transformer: Reinforcement Learning via Sequence Modeling</a><br>
<a href="https://arxiv.org/abs/2103.05247">Pretrained Transformers as Universal Computation Engines</a><br>
<a href="https://arxiv.org/abs/2205.14459">CyCLIP: Cyclic Contrastive Language-Image Pretraining</a><br>
<a href="https://arxiv.org/pdf/2211.11319">VectorFusion: Text-to-SVG by Abstracting Pixel-Based Diffusion Models</a><br>
<a href="https://arxiv.org/abs/2212.03995"> ! Not ML ! Phase Coexistence Implications of Violating Newton's Third Law</a><br>
<a href="https://arxiv.org/pdf/2206.00888">Squeezeformer: An Efficient Transformer for Automatic Speech Recognition</a><br>
<a href="https://arxiv.org/pdf/2203.06211">Staged Training for Transformer Language Models</a><br>
<a href="https://arxiv.org/pdf/2210.12529">On-Demand Sampling:Learning Optimally from Multiple Distributions</a><br>
<a href="https://arxiv.org/abs/2211.12740">Masked Autoencoding for Scalable and Generalizable Decision Making</a><br>
<a href="https://arxiv.org/pdf/1510.00149">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</a><br>
<a href="https://arxiv.org/abs/2010.11929">* An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a><br>
<a href="https://arxiv.org/pdf/2205.14217">Diffusion-LM Improves Controllable Text Generation</a><br>
<a href="https://arxiv.org/pdf/2106.09226">Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning</a><br>
<a href="https://arxiv.org/pdf/2207.07635">Is a Caption Worth a Thousand Images? A Controlled Study for Representation Learning</a><br>
<a href="https://arxiv.org/pdf/2210.09338">Deep Bidirectional Language-Knowledge Graph Pretraining</a><br>
<a href="https://arxiv.org/pdf/2208.01066">What Can Transformers Learn In-Context? A Case Study of Simple Function Classes</a><br>
<a href="https://arxiv.org/pdf/2209.06235">Improving Self-Supervised Learning by Characterizing Idealized Representations</a><br>
<a href="https://arxiv.org/pdf/2104.05694">On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies</a><br>
<a href="https://arxiv.org/pdf/2107.09285">Neural Abstructions: Abstractions that Support Construction for Grounded Language Learning</a><br>
<a href="https://arxiv.org/pdf/2110.05722">LightSeq2: Accelerated Training for Transformer-based Models on GPUs</a><br>
<a href="https://arxiv.org/pdf/2002.03428">Improving Neural Network Learning Through Dual Variable Learning Rates</a><br>
<a href="https://arxiv.org/abs/2205.10674">NS3: Neuro-Symbolic Semantic Code Search</a><br>
<a href="https://arxiv.org/pdf/1811.08888.pdf">Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks</a><br>
<a href="https://arxiv.org/pdf/1905.13210.pdf">Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks</a><br>
<br>
November:<br>
November was a month of travel and reflection<br>
<br>
October:<br>
<a href="https://arxiv.org/pdf/2210.09520">Using Language to Extend to Unseen Domains</a><br>
<a href="https://arxiv.org/pdf/1905.11881">Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity</a><br>
<a href="https://arxiv.org/pdf/1804.02767">* YOLOv3: An Incremental Improvement</a><br>
<a href="https://arxiv.org/pdf/2010.11929">* An Image Is Worth 16X16 Words: Transformers for Image Recognition at Scale</a><br>
<a href="https://arxiv.org/pdf/2205.01917v2">CoCa: Contrastive Captioners are Image-Text Foundation Models</a><br>
<a href="https://arxiv.org/pdf/2106.04560v2">Scaling Vision Transformers</a><br>
<a href="https://arxiv.org/pdf/2106.04803v2">* CoAtNet: Marrying Convolution and Attention
for All Data Sizes</a><br>
<a href="https://arxiv.org/pdf/2209.06794v2">PaLI: A Jointly-Scaled Multilingual Language-Image Model</a><br>
<a href="https://arxiv.org/abs/1904.00962">* Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</a><br>
<a href="https://arxiv.org/pdf/2112.05682">Self-Attention Does Not Need O(n^2) Memory</a><br>
<a href="https://arxiv.org/pdf/1910.02054">* ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a><br>
<a href="https://arxiv.org/pdf/2103.00020">* Learning Transferable Visual Models From Natural Language Supervision</a><br>
<a href="https://arxiv.org/abs/1502.05767">Automatic Differentiation in Machine Learning: a Survey</a><br>
<br>
September:<br>
<a href="https://arxiv.org/pdf/2207.09238">* Formal Algorithms for Transformers</a><br>
<a href="https://arxiv.org/pdf/2102.07074v2">TransGAN: Two Transformers Can Make One Strong GAN</a><br>
<a href="https://arxiv.org/pdf/1810.04805">* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a><br>
<a href="https://arxiv.org/pdf/2108.13342">DNNFusion: Accelerating Deep Neural Networks Execution with Advanced Operator Fusion</a><br>
<a href="https://arxiv.org/pdf/1905.05957">DoubleSqueeze: Parallel Stochastic Gradient Descent with Double-pass Error-Compensated Compression</a><br>
<a href="https://arxiv.org/pdf/2102.02888">1-bit Adam: Communication Efficient Large-Scale Training with Adamâ€™s
Convergence Speed</a><br>
<a href="https://arxiv.org/pdf/2104.06069">* 1-Bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMBâ€™s Convergence Speed</a><br>
<a href="https://arxiv.org/pdf/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a><br>
<a href="https://arxiv.org/pdf/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a><br>
<a href="https://arxiv.org/pdf/1412.6980.">* Adam: A Method for Stochastic Optimization</a><br>
<a href="https://arxiv.org/pdf/1711.05101">Decoupled Weight Decay Regularization</a><br>
<a href="https://arxiv.org/pdf/2104.07857">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</a><br>
<a href="https://arxiv.org/pdf/2104.00298">EfficientNetV2: Smaller Models and Faster Training</a><br>
<a href="https://arxiv.org/pdf/1905.11946">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a><br>
<a href="https://arxiv.org/pdf/1812.00332">ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware</a><br>
<a href="https://arxiv.org/pdf/1512.03385">* Deep Residual Learning for Image Recognition</a><br>
<a href="https://arxiv.org/pdf/2202.01169">Unified Scaling Laws for Routed Language Models</a><br>
<a href="https://arxiv.org/pdf/2006.16668">GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</a><br>
<a href="https://arxiv.org/pdf/1701.06538">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a><br>
<a href="https://arxiv.org/pdf/1905.07830v1">HellaSwag: Can a Machine Really Finish Your Sentence?</a><br>
<a href="https://arxiv.org/pdf/1707.06347">Proximal Policy Optimization Algorithms</a><br>
<a href="https://arxiv.org/pdf/1509.02971">Continuous Control With Deep Reinforcement Learning</a><br>
<a href="https://arxiv.org/pdf/1906.11046">Multi-Agent Deep Reinforcement Learning for Liquidation Strategy Analysis</a><br>
<a href="https://arxiv.org/pdf/1811.07522">Practical Deep Reinforcement Learning Approach for Stock Trading</a><br>
<a href="https://arxiv.org/pdf/1911.08265">* Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model</a><br>
<a href="https://arxiv.org/pdf/1905.12941">Learning Compositional Neural Programs with Recursive Tree Search and Planning</a><br>
<a href="https://arxiv.org/abs/2111.03904">On Pseudo-Absence Generation and Machine Learning for Locust Breeding Ground Prediction in Africa</a><br>
<a href="https://arxiv.org/abs/2202.03057">Multi-Objective Quality Diversity Optimization</a><br>
<a href="https://arxiv.org/pdf/1709.01507">Squeeze-and-Excitation Networks</a><br>
<a href="https://arxiv.org/pdf/1705.08741">Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training of Neural Networks</a><br>
<a href="https://arxiv.org/pdf/1807.11626">MnasNet: Platform-Aware Neural Architecture Search for Mobile</a><br>
<a href="https://arxiv.org/pdf/2203.15556">* Training Compute-Optimal Large Language Models</a><br>
<a href="https://arxiv.org/abs/2103.12731">Scaling Local Self-Attention for Parameter Efficient Visual Backbones</a><br>
<a href="https://arxiv.org/pdf/2112.11446">Scaling Language Models: Methods, Analysis & Insights from Training Gopher</a><br>
<a href="https://arxiv.org/pdf/1909.11942">ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations</a><br>
<a href="https://arxiv.org/pdf/2112.04426">Improving Language Models by Retrieving From Trillions of Tokens</a><br>
<a href="https://arxiv.org/pdf/2108.07732">Program Synthesis with Large Language Models</a><br>
<a href="https://arxiv.org/pdf/2005.08100">Conformer: Convolution-Augmented Transformer for Speech Recognition</a><br>
<a href="https://arxiv.org/pdf/1706.03762">* Attention Is All You Need</a><br>
<a href="https://arxiv.org/pdf/1712.05245">Pointwise Convolutional Neural Networks</a><br>
<a href="https://arxiv.org/pdf/2002.05202">GLU Variants Improve Transformer</a><br>
<a href="https://arxiv.org/pdf/1803.02155">Self-Attention with Relative Position Representations</a><br>
<a href="https://arxiv.org/pdf/1906.05909">Stand-Alone Self-Attention in Vision Models</a><br>
<a href="https://arxiv.org/abs/1802.05751">Image Transformer</a><br>
<a href="https://arxiv.org/pdf/1811.02084">Mesh-TensorFlow: Deep Learning for Supercomputers</a><br>
<a href="https://arxiv.org/abs/1409.1259">On the Properties of Neural Machine Translation: Encoder-Decoder Approaches</a><br>
<a href="https://arxiv.org/pdf/1506.02626">Learning both Weights and Connections for Efficient Neural Networks</a><br>
<a href="https://arxiv.org/pdf/1510.00149">Deep Compression: Compressing Deep Neural Networks With Pruning, Trained Quantization and Huffman Coding</a><br>
<a href="https://arxiv.org/abs/2110.11499">Wav2CLIP: Learning Robust Audio Representations From CLIP</a><br>
<a href="https://arxiv.org/pdf/2112.04323">* Contrastive Learning with Large Memory Bank and Negative Embedding Subtraction for Accurate Copy Detection</a><br>
<a href="https://arxiv.org/abs/1801.01442">ObamaNet: Photo-Realistic Lip-Sync From Text</a><br>
<a href="https://arxiv.org/abs/2206.10369">The State of Sparse Training in Deep Reinforcement Learning</a><br>
<a href="https://arxiv.org/abs/2006.08505">Diversity Policy Gradient for Sample Efficient Quality-Diversity Optimization</a><br>
<a href="https://arxiv.org/abs/2110.10139">Chunked Autoregressive GAN for Conditional Waveform Synthesis</a><br>
<a href="https://arxiv.org/abs/1910.06711">MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis</a><br>
<a href="https://arxiv.org/abs/1612.07837">SampleRNN: An Unconditional End-to-End Neural Audio Generation Model</a><br>
<br>

</p>
</body>
</html>
