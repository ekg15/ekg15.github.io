<!DOCTYPE html>
<html>
<body>
<p>Ethan Goldfarb<br>
ethankgoldfarb@gmail.com</p>

<p>here's a list of the papers I've been reading recently:<br>
<p style="font-size: 10px">Note: what "read" in this instance constitutes ranges from "skimmed" to "fully annotated". Ask me if curious about a particular paper.<br>
* indicates I liked a paper or found it notable. Lack of a * does not indicate otherwise.<br></p>
<br>
2023:<br>
<br>
February:<br>
<br>
<a href="https://arxiv.org/abs/2301.11325">MusicLM: Generating Music From Text</a><br>
<a href="https://arxiv.org/pdf/2006.11477.pdf">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</a><br>
<a href="https://arxiv.org/pdf/2202.05607.pdf">Online Decision Transformer</a><br>
<a href="https://arxiv.org/pdf/2206.04850.pdf">Feature-informed Embedding Space Regularization For Audio Classification</a><br>
<a href="https://arxiv.org/pdf/1904.05862.pdf">Wav2vec: Unsupervised Pre-Training for Speech Recognition</a><br>
<a href="https://ai.facebook.com/research/publications/efficient-self-supervised-learning-with-contextualized-target-representations-for-vision-speech-and-language">Efficient Self-supervised Learning with Contextualized Target Representations</a><br>

for Vision, Speech and Language
<br>
January:<br>
<br>
<a href="https://arxiv.org/pdf/2205.11487.pdf">Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</a><br>
<a href="https://arxiv.org/pdf/2211.01324.pdf">eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers</a><br>
<a href="https://arxiv.org/pdf/2112.10752.pdf">High-Resolution Image Synthesis with Latent Diffusion Models</a><br>
<a href="https://arxiv.org/pdf/2105.05233.pdf">Diffusion Models Beat GANs on Image Synthesis</a><br>
<a href="https://arxiv.org/pdf/1804.08838.pdf">Measuring the Intrinsic Dimension of Objective Landscapes</a><br>
<a href="https://arxiv.org/pdf/1911.12360.pdf">How Much Over-Parameterization is Sufficient to Learn Deep ReLu Networks?</a><br>
<a href="https://arxiv.org/pdf/2210.14199.pdf">Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models</a><br>
<a href="https://arxiv.org/pdf/2211.14699.pdf">A Theoretical Study of Inductive Biases in Contrastive Learning</a><br>
<a href="https://arxiv.org/pdf/2206.10139.pdf">Insights into Pre-training via Simpler Synthetic Tasks</a><br>
<a href="https://arxiv.org/abs/1601.01280">Language to Logical Form with Neural Attention</a><br>
<a href="https://arxiv.org/pdf/2201.07348.pdf">Learning Tensor Representations for Meta-Learning</a><br>
<a href="https://arxiv.org/pdf/2105.05233.pdf">Diffusion Models Beat GANs on Image Synthesis</a><br>
<a href="https://arxiv.org/pdf/2206.08853.pdf">MINEDOJO: Building Open-Ended Embodied Agents with Internet-Scale Knowledge</a><br>
<a href="https://openreview.net/pdf?id=M3Y74vmsMcY">LAION-5B: An open large-scale dataset for training next generation image-text models</a><br">
<a href="https://arxiv.org/pdf/1503.03585.pdf">* Deep Unsupervised Learning using Nonequilibrium Thermodynamics</a><br>
<a href="https://arxiv.org/pdf/2106.02039.pdf">Offline Reinforcement Learning as One Big Sequence Modeling Problem</a><br>
<a href="https://arxiv.org/pdf/2112.10752.pdf">High-Resolution Image Synthesis with Latent Diffusion Models</a><br>
<a href="https://arxiv.org/pdf/2204.06125.pdf">Hierarchical Text-Conditional Image Generation with CLIP Latents</a><br>
<br>
<br>
2022:<br>
<br>
December:<br>
<br>
<a href="https://arxiv.org/abs/2108.04552">The Benefits of Implicit Regularization from SGD in Least Squares Problems</a><br>
<a href="https://arxiv.org/abs/2106.01345">* Decision Transformer: Reinforcement Learning via Sequence Modeling</a><br>
<a href="https://arxiv.org/abs/2103.05247">Pretrained Transformers as Universal Computation Engines</a><br>
<a href="https://arxiv.org/abs/2205.14459">CyCLIP: Cyclic Contrastive Language-Image Pretraining</a><br>
<a href="https://arxiv.org/pdf/2211.11319">VectorFusion: Text-to-SVG by Abstracting Pixel-Based Diffusion Models</a><br>
<a href="https://arxiv.org/abs/2212.03995"> ! Not ML ! Phase Coexistence Implications of Violating Newton's Third Law</a><br>
<a href="https://arxiv.org/pdf/2206.00888">Squeezeformer: An Efficient Transformer for Automatic Speech Recognition</a><br>
<a href="https://arxiv.org/pdf/2203.06211">Staged Training for Transformer Language Models</a><br>
<a href="https://arxiv.org/pdf/2210.12529">On-Demand Sampling:Learning Optimally from Multiple Distributions</a><br>
<a href="https://arxiv.org/abs/2211.12740">Masked Autoencoding for Scalable and Generalizable Decision Making</a><br>
<a href="https://arxiv.org/pdf/1510.00149">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</a><br>
<a href="https://arxiv.org/abs/2010.11929">* An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a><br>
<a href="https://arxiv.org/pdf/2205.14217">Diffusion-LM Improves Controllable Text Generation</a><br>
<a href="https://arxiv.org/pdf/2106.09226">Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning</a><br>
<a href="https://arxiv.org/pdf/2207.07635">Is a Caption Worth a Thousand Images? A Controlled Study for Representation Learning</a><br>
<a href="https://arxiv.org/pdf/2210.09338">Deep Bidirectional Language-Knowledge Graph Pretraining</a><br>
<a href="https://arxiv.org/pdf/2208.01066">What Can Transformers Learn In-Context? A Case Study of Simple Function Classes</a><br>
<a href="https://arxiv.org/pdf/2209.06235">Improving Self-Supervised Learning by Characterizing Idealized Representations</a><br>
<a href="https://arxiv.org/pdf/2104.05694">On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies</a><br>
<a href="https://arxiv.org/pdf/2107.09285">Neural Abstructions: Abstractions that Support Construction for Grounded Language Learning</a><br>
<a href="https://arxiv.org/pdf/2110.05722">LightSeq2: Accelerated Training for Transformer-based Models on GPUs</a><br>
<a href="https://arxiv.org/pdf/2002.03428">Improving Neural Network Learning Through Dual Variable Learning Rates</a><br>
<a href="https://arxiv.org/abs/2205.10674">NS3: Neuro-Symbolic Semantic Code Search</a><br>
<a href="https://arxiv.org/pdf/2112.04323">* Contrastive Learning with Large Memory Bank and Negative Embedding Subtraction for Accurate Copy Detection</a><br>
<a href="https://arxiv.org/pdf/1811.08888.pdf">Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks</a><br>
<a href="https://arxiv.org/pdf/1905.13210.pdf">Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks</a><br>
<br>
November:<br>
<br>
<a href="https://arxiv.org/pdf/2112.00114">* Show Your Work: Scratchpads for Intermediate Computation with Language Models</a><br>
<br>
October:<br>
<a href="https://arxiv.org/pdf/2210.09520">Using Language to Extend to Unseen Domains</a><br>
<a href="https://arxiv.org/pdf/1905.11881">Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity</a><br>
<a href="https://arxiv.org/pdf/2104.06069">* 1-Bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMBâ€™s Convergence Speed</a><br>
<a href="https://arxiv.org/pdf/2203.15556">* Training Compute-Optimal Large Language Models</a><br>
<a href="https://arxiv.org/pdf/1804.02767">* YOLOv3: An Incremental Improvement</a><br>
<a href="https://arxiv.org/pdf/2010.11929">* An Image Is Worth 16X16 Words: Transformers for Image Recognition at Scale</a><br>
<a href="https://arxiv.org/pdf/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a><br>
<a href="https://arxiv.org/pdf/2207.09238">* Formal Algorithms for Transformers</a><br>
<a href="https://arxiv.org/pdf/2205.01917v2">CoCa: Contrastive Captioners are Image-Text Foundation Models</a><br>
<a href="https://arxiv.org/pdf/2106.04560v2">Scaling Vision Transformers</a><br>
<a href="https://arxiv.org/pdf/2106.04803v2">* CoAtNet: Marrying Convolution and Attention
for All Data Sizes</a><br>
<a href="https://arxiv.org/pdf/2209.06794v2">PaLI: A Jointly-Scaled Multilingual Language-Image Model</a><br>
<a href="https://arxiv.org/abs/2103.12731">Scaling Local Self-Attention for Parameter Efficient Visual Backbones</a><br>
<a href="https://arxiv.org/pdf/1706.03762">* Attention Is All You Need</a><br>
<a href="https://arxiv.org/abs/1904.00962">* Large Batch Optimization for Deep Learning: Training BERT in 76 minutes</a><br>
<a href="https://arxiv.org/pdf/2112.05682">Self-Attention Does Not Need O(n^2) Memory</a><br>
<a href="https://arxiv.org/pdf/1412.6980.">* Adam: A Method for Stochastic Optimization</a><br>
<a href="https://arxiv.org/pdf/1910.02054">* ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a><br>
<a href="https://arxiv.org/pdf/2103.00020">* Learning Transferable Visual Models From Natural Language Supervision</a><br>
<a href="https://arxiv.org/abs/1502.05767">Automatic Differentiation in Machine Learning: a Survey</a><br>

<br>

</p>
</body>
</html>
