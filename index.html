<!DOCTYPE html>
<html>
<body>
<p>Ethan Goldfarb<br>
ethankgoldfarb@gmail.com</p>

<p>here's a list of the papers I've been reading recently:<br>
<br>
2023:<br>
<br>
February<br>
<br>
<a href="https://www.google.com/url?q=https://arxiv.org/abs/2301.11325&usg=AOvVaw3GOteWW3RR6wZ6mUswZUvs">[2301.11325] MusicLM: Generating Music From Text</a><br>
<a href="https://www.google.com/url?q=https://arxiv.org/pdf/2006.11477.pdf&usg=AOvVaw1Gpl5_Mr79qIMQzwseS6qI">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</a><br>
<a href="https://arxiv.org/pdf/2205.11487.pdf">Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</a><br>
<a href="https://arxiv.org/pdf/2211.01324.pdf">eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers</a><br>
<a href="https://arxiv.org/pdf/2112.10752.pdf">High-Resolution Image Synthesis with Latent Diffusion Models</a><br>
<a href="https://arxiv.org/pdf/2105.05233.pdf">Diffusion Models Beat GANs on Image Synthesis</a><br>
<a href="https://arxiv.org/pdf/1804.08838.pdf">Measuring the Intrinsic Dimension of Objective Landscapes</a><br>
<a href="https://arxiv.org/pdf/2103.05247">Pretrained Transformers as Universal Computation Engines</a><br>
<a href="https://arxiv.org/pdf/2202.05607.pdf">Online Decision Transformer</a><br>
<br>
January<br>
<a href="https://arxiv.org/pdf/2202.05607.pdf">Online Decision Transformer</a><br>
<a href="https://arxiv.org/pdf/1811.08888.pdf">Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks</a><br>
<a href="https://arxiv.org/pdf/1905.13210.pdf">Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks</a><br>
<a href="https://arxiv.org/pdf/1911.12360.pdf">How Much Over-Parameterization is Sufficient to Learn Deep ReLu Networks?</a><br>
<a href="https://arxiv.org/pdf/2210.14199.pdf">Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models</a><br>
<a href="https://arxiv.org/pdf/2211.14699.pdf">A Theoretical Study of Inductive Biases in Contrastive Learning</a><br>
<a href="https://arxiv.org/pdf/2206.10139.pdf">Insights into Pre-training via Simpler Synthetic Tasks</a><br>
<a href="https://arxiv.org/abs/1601.01280">Language to Logical Form with Neural Attention</a><br>
<a href="https://arxiv.org/pdf/2201.07348.pdf">Learning Tensor Representations for Meta-Learning</a><br>
<a href="https://arxiv.org/pdf/2105.05233.pdf">Diffusion Models Beat GANs on Image Synthesis</a><br>
<a href="https://arxiv.org/pdf/2206.08853.pdf">MINEDOJO: Building Open-Ended Embodied Agents with Internet-Scale Knowledge</a><br>
<a href="https://openreview.net/pdf?id=M3Y74vmsMcY"LAION-5B: An open large-scale dataset for training next generation image-text models</a><br">
<a href="https://arxiv.org/pdf/1503.03585.pdf">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</a><br>
<a href="https://arxiv.org/pdf/2106.02039.pdf">Offline Reinforcement Learning as One Big Sequence Modeling Problem</a><br>

</p>
</body>
</html>
